{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "faab0ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.ml.classification import MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.sql.types import LongType, DoubleType\n",
    "from pyspark.ml.linalg import DenseVector\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f0a557a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.master(\"spark://master:7077\").appName(\"Multilayer perceptron classifier\").config(\"spark.executor.memory\", \"6gb\").getOrCreate()\n",
    "#spark.config(\"spark.executor.memory\", \"6gb\")\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7adcefa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['labels']\n",
    "for i in range(1, 1025):\n",
    "    columns.append(\"f\" + str(i))\n",
    "df = spark.read.format('csv').options(header='true').load('/MLInput_u/MLInput_u.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49846aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import functions as F\n",
    "\n",
    "# def sum_col(df, col):\n",
    "#     return df.select(F.sum(col)).collect()[0][0]\n",
    "\n",
    "# for i in range(1, 1025):\n",
    "#     colname = \"f\" + str(i)\n",
    "#     s = sum_col(df, colname)\n",
    "    \n",
    "#     if s < 1: # Equal to if s is approx 0, but avoiding numerical errors\n",
    "#         print(colname)\n",
    "        \n",
    "#     if i % 25 == 0:\n",
    "#         print(\"done until \" + str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "565f5643",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertColumn(df, names, newType):\n",
    "    for name in names: \n",
    "        df = df.withColumn(name, df[name].cast(newType))\n",
    "    return df \n",
    "\n",
    "df = convertColumn(df, columns, LongType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc65c76e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- label: long (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    1|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    1|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,1.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    1|[0.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    1|[1.0,0.0,0.0,0.0,...|\n",
      "|    0|[0.0,0.0,0.0,0.0,...|\n",
      "|    1|[0.0,0.0,0.0,0.0,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_data = df.rdd.map(lambda x: (x[0], DenseVector(x[1:])))\n",
    "df_new = spark.createDataFrame(input_data, [\"label\", \"features\"])\n",
    "\n",
    "# standardScaler = StandardScaler(inputCol=\"features\", outputCol=\"features_scaled\")\n",
    "# scaler = standardScaler.fit(df_new)\n",
    "# scaled_df = scaler.transform(df_new)\n",
    "\n",
    "# scaled_df.drop('features')\n",
    "\n",
    "scaled_df = df_new\n",
    "\n",
    "scaled_df.take(2)\n",
    "scaled_df.printSchema()\n",
    "scaled_df.show()\n",
    "\n",
    "# #Not Scaling\n",
    "# scaled_df = df_new\n",
    "# scaled_df.printSchema()\n",
    "# scaled_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d95e24ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = scaled_df.randomSplit([0.6, 0.4], 1234)\n",
    "train = splits[0]\n",
    "test = splits[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ae8252",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "maxIter = 100\n",
    "layers = [[1024, 50, 2], [1024, 100, 2], [1024, 150, 2]]\n",
    "\n",
    "trainer = MultilayerPerceptronClassifier(maxIter=maxIter)\n",
    "paramGrid = ParamGridBuilder().addGrid(trainer.layers, layers).build()\n",
    "\n",
    "crossval = CrossValidator(estimator=trainer,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=BinaryClassificationEvaluator(),\n",
    "                          numFolds=3)\n",
    "\n",
    "model = crossval.fit(train)\n",
    "result = model.transform(test)\n",
    "timeConsumed = time.time() - start_time\n",
    "predictionAndLabels = result.select(\"prediction\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = model.bestModel\n",
    "best_reg_param = best_model._java_obj.getRegParam()\n",
    "best_elasticnet_param = best_model._java_obj.getElasticNetParam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9ffb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Perceptron_Result', 'w') as f:\n",
    "    f.write(str(best_reg_param))\n",
    "    f.write('\\n')\n",
    "    f.write(str(best_elasticnet_param))\n",
    "    f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214c7c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "columns = [\"prediction\", \"label\"]\n",
    "predictionAndLabels = convertColumn(predictionAndLabels, columns, DoubleType())\n",
    "metrics = MulticlassMetrics(predictionAndLabels.rdd)\n",
    "\n",
    "cfMatrix = metrics.confusionMatrix().toArray()\n",
    "precision = metrics.precision()\n",
    "recall = metrics.recall()\n",
    "f1Score = metrics.fMeasure()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator()\n",
    "evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "accuracy = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"f1\"})\n",
    "weightedPrecision = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"weightedPrecision\"})\n",
    "weightedRecall = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"weightedRecall\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ccad45",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Perceptron_Result', 'a') as f:\n",
    "    f.write('Summary Stats:' + '\\n')\n",
    "    f.write('It takes %s minutes' % timeConsumed)\n",
    "    f.write('\\n')\n",
    "    f.write(str(metrics.confusionMatrix().toArray()))\n",
    "    f.write('\\n')\n",
    "    f.write('Precision: %s \\n' % precision)\n",
    "    f.write('Recall: %s \\n' % recall)\n",
    "    f.write('f1Score: %s \\n'% f1Score)\n",
    "    f.write('Accuracy: %s \\n' % accuracy)\n",
    "    f.write('f1: %s \\n' % f1)\n",
    "    f.write('Weighted Precision: %s \\n' % weightedPrecision)\n",
    "    f.write('weighted Recall: %s \\n'% weightedRecall)\n",
    "    \n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64465ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# middle = 60\n",
    "# maxIter = 100\n",
    "# metrics = {}\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# layers = [1024, middle, 2]\n",
    "# trainer = MultilayerPerceptronClassifier(maxIter=maxIter, layers=layers, blockSize=128, seed=1234)\n",
    "\n",
    "# model = trainer.fit(train)\n",
    "# result = model.transform(test)\n",
    "# predictionAndLabels = result.select(\"prediction\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb1631a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "# columns = [\"prediction\", \"label\"]\n",
    "# predictionAndLabels = convertColumn(predictionAndLabels, columns, DoubleType())\n",
    "# metrics = MulticlassMetrics(predictionAndLabels.rdd)\n",
    "# print(metrics.confusionMatrix().toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7b0bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluator = BinaryClassificationEvaluator()\n",
    "# evaluator.setPredictionCol(\"prediction\")\n",
    "\n",
    "# accuracy = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"accuracy\"})\n",
    "# print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a33458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1 = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"f1\"})\n",
    "# print(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17634d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightedPrecision = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"weightedPrecision\"})\n",
    "# print(weightedPrecision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdf0163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightedRecall = evaluator.evaluate(predictionAndLabels, {evaluator.metricName: \"weightedRecall\"})\n",
    "# print(weightedRecall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b1d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictionAndLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d482cc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision = metrics.precision()\n",
    "# print(\"Summary Stats\")\n",
    "# print(\"Precision = %s\" % precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d455161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall = metrics.recall()\n",
    "# print(\"Recall = %s\" % recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e277fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1Score = metrics.fMeasure()\n",
    "# print(\"F1 Score = %s\" % f1Score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea5d0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Weighted recall = %s\" % metrics.weightedRecall)\n",
    "# print(\"Weighted precision = %s\" % metrics.weightedPrecision)\n",
    "# print(\"Weighted F(1) Score = %s\" % metrics.weightedFMeasure())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13d136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark.stop()\n",
    "# sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
